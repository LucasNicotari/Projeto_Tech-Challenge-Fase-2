# Projeto Tech Challenge - Pipeline de Dados B3

Pipeline automatizado para extraÃ§Ã£o, processamento e armazenamento de dados do IBOVESPA da B3 (Brasil, Bolsa, BalcÃ£o).

## ğŸ“‹ DescriÃ§Ã£o

Este projeto implementa um pipeline completo de dados que:
- Faz web scraping dos dados do IBOVESPA diretamente do site da B3
- Processa e limpa os dados extraÃ­dos
- Converte para formato Parquet otimizado para anÃ¡lise
- Faz upload automÃ¡tico para Amazon S3
- Inclui logging completo e tratamento de erros

## ğŸš€ Funcionalidades

- **Web Scraping Automatizado**: ExtraÃ§Ã£o de dados diretamente do site da B3
- **Processamento Inteligente**: Limpeza e formataÃ§Ã£o dos dados
- **DetecÃ§Ã£o de Dias NÃ£o Ãšteis**: Automaticamente baixa dados do prÃ³ximo dia Ãºtil
- **Formato Otimizado**: ConversÃ£o para Parquet para melhor performance
- **Upload S3**: Armazenamento automÃ¡tico na nuvem AWS
- **Logging Completo**: Rastreamento detalhado de todas as operaÃ§Ãµes
- **Tratamento de Erros**: RecuperaÃ§Ã£o automÃ¡tica de falhas

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.8+**
- **Selenium**: Web scraping automatizado
- **Pandas**: ManipulaÃ§Ã£o e processamento de dados
- **PyArrow**: GeraÃ§Ã£o de arquivos Parquet
- **Boto3**: IntegraÃ§Ã£o com AWS S3
- **Chrome WebDriver**: AutomaÃ§Ã£o do navegador

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/Projeto-Tech-Challenge.git
cd Projeto-Tech-Challenge
```

2. Instale as dependÃªncias:
```bash
pip install -r requirements_file.txt
```

3. Configure as variÃ¡veis de ambiente:
```bash
cp .env.example .env
# Edite o arquivo .env com suas credenciais AWS
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Crie um arquivo `.env` com as seguintes variÃ¡veis:

```env
# AWS Configuration
AWS_ACCESS_KEY_ID=sua_access_key
AWS_SECRET_ACCESS_KEY=sua_secret_key
AWS_SESSION_TOKEN=seu_session_token  # Opcional
AWS_REGION=us-east-1
S3_BUCKET_NAME=seu-bucket-name

# Pipeline Settings
LOG_LEVEL=INFO
CLEANUP_DAYS=7
SELENIUM_TIMEOUT=20
```

### PrÃ©-requisitos

- Chrome/Chromium instalado
- ChromeDriver no PATH do sistema
- Credenciais AWS configuradas
- Bucket S3 criado

## ğŸš€ Uso

### ExecuÃ§Ã£o BÃ¡sica

```bash
# Executar pipeline para data atual
python main_pipeline.py

# Executar para data especÃ­fica
python main_pipeline.py --date 2025-08-03

# Pular download se arquivo jÃ¡ existir
python main_pipeline.py --skip-download

# Pular upload para S3
python main_pipeline.py --skip-upload

# Limpar arquivos antigos apÃ³s execuÃ§Ã£o
python main_pipeline.py --cleanup
```

### MÃ³dulos Individuais

```bash
# Apenas fazer scraping
python scraper_module.py

# Apenas processar dados existentes
python data_processor_module.py

# Apenas fazer upload
python s3_uploader_module.py
```

## ğŸ“ Estrutura do Projeto

```
Projeto-Tech-Challenge/
â”œâ”€â”€ main_pipeline.py          # Pipeline principal
â”œâ”€â”€ scraper_module.py         # Web scraping da B3
â”œâ”€â”€ data_processor_module.py  # Processamento de dados
â”œâ”€â”€ s3_uploader_module.py     # Upload para S3
â”œâ”€â”€ config_module.py          # ConfiguraÃ§Ãµes centralizadas
â”œâ”€â”€ logger_module.py          # Sistema de logging
â”œâ”€â”€ env_loader.py             # Carregamento de variÃ¡veis
â”œâ”€â”€ requirements_file.txt     # DependÃªncias Python
â”œâ”€â”€ .gitignore               # Arquivos ignorados pelo Git
â”œâ”€â”€ README.md                # DocumentaÃ§Ã£o
â”œâ”€â”€ downloads/               # Arquivos CSV baixados
â”œâ”€â”€ data/                    # Arquivos Parquet processados
â””â”€â”€ logs/                    # Logs de execuÃ§Ã£o
```

## ğŸ“Š Dados Processados

O pipeline processa os seguintes campos:

- **codigo**: CÃ³digo da aÃ§Ã£o (ex: PETR4, VALE3)
- **acao**: Nome da empresa
- **tipo**: Tipo de aÃ§Ã£o (ON, PN, etc.)
- **qtdeteorica**: Quantidade teÃ³rica de aÃ§Ãµes
- **part**: ParticipaÃ§Ã£o percentual no Ã­ndice
- **data_pregao**: Data do pregÃ£o (formato dd/mm/aaaa)
- **timestamp_extracao**: Timestamp da extraÃ§Ã£o (ISO format)

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### PersonalizaÃ§Ã£o de Caminhos S3

O arquivo Ã© salvo em: `s3://bucket-name/raw/parquet file/arquivo.parquet`

### ConfiguraÃ§Ã£o de Timeout

Ajuste o timeout do Selenium no arquivo `.env`:
```env
SELENIUM_TIMEOUT=30  # segundos
```

### Limpeza AutomÃ¡tica

Configure quantos dias de arquivos manter:
```env
CLEANUP_DAYS=7  # manter arquivos por 7 dias
```

## ğŸ“ Logs

Os logs sÃ£o salvos em `logs/` com rotaÃ§Ã£o diÃ¡ria:
- `B3DataPipeline_YYYYMMDD.log`: Log principal do pipeline
- `B3Scraper_YYYYMMDD.log`: Log do web scraping
- `DataProcessor_YYYYMMDD.log`: Log do processamento
- `S3Uploader_YYYYMMDD.log`: Log do upload S3

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido como parte do Tech Challenge - PÃ³s-graduaÃ§Ã£o FIAP

## ğŸ†˜ Suporte

Para suporte e dÃºvidas:
- Abra uma issue no GitHub
- Consulte os logs em `logs/` para diagnÃ³stico
- Verifique a configuraÃ§Ã£o das variÃ¡veis de ambiente
